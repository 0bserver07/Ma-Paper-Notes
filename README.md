**Brief Stacked 2 layered notes on papers around Machine Learning can be found below:**

----

* :penguin:'s mean notes are included in repo.



----


#### Rest of Fall-2016:

* [Implicit ReasoNet: Modeling Large-Scale Structured Relationships with Shared Memory](https://github.com/0bserver07/Mah-Paper-Notes/blob/master/notes/Implicit-ReasoNet.md) <:penguin:> [Paper](https://arxiv.org/abs/1611.04642)
* Neural Turing Machines: < > [(arXiv:1410.5401)](https://arxiv.org/abs/1410.5401)
* Differentiable Neural Computer: < > [DeepMind /on Nature](https://deepmind.com/blog/differentiable-neural-computers/)
* Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes: < > [(arXiv:1610.09027)](https://arxiv.org/abs/1610.09027)



#### October-Nov of Fall-2016:

* [Fully Character-Level Neural Machine Translation without Explicit Segmentation] (https://github.com/0bserver07/Mah-Paper-Notes/blob/master/notes/Fully%20Character-Level%20Neural%20Machine%20Translation%20without%20Explicit%20Segmentation.md) <:penguin:> [[Paper](https://arxiv.org/abs/1610.03017v1)]

* Using Fast Weights to Attend to the Recent Past < > [[Paper](https://arxiv.org/abs/1610.06258)]
* Connecting Generative Adversarial Networks and Actor-Critic Methods < > [[Paper](https://arxiv.org/abs/1610.01945)]
* SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient < > [[Paper](https://arxiv.org/abs/1609.05473)]
* Character-Aware Neural Language Models < > [[Paper](https://arxiv.org/abs/1508.06615)]
* Memory-enhanced Decoder for Neural Machine Translation < > [[Paper](https://arxiv.org/abs/1606.02003)]
* Structured Generative Models of Natural Source Code < > [[Paper](https://arxiv.org/abs/1401.0514)]
* Learning to Reason With Adaptive Computation < > [[Paper](https://arxiv.org/abs/1610.07647)]
* Iterative Refinement for Machine Translation < > [[Paper](https://arxiv.org/abs/1610.06602)]


#### Sep-2016:

* Latent Predictor Networks for Code Generation < > [[Paper](https://arxiv.org/abs/1603.06744)]
* Neural Machine Translation with Supervised Attention < > [[Paper](https://arxiv.org/abs/1609.04186)]
* Hybrid computing using a neural network with dynamic external memory < > [[Paper](http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz)] *todo*
* Attend, Infer, Repeat: Fast Scene Understanding with Generative Models < > [[Paper](https://arxiv.org/abs/1603.08575)] *todo*

* Recurrent Highway Networks < > [[Paper](https://arxiv.org/abs/1607.03474)]
* Hierarchical Multiscale Recurrent Neural Networks < > [[Paper](https://arxiv.org/abs/1609.01704)]
* Unifying Count-Based Exploration and Intrinsic Motivation < > [[Paper](https://arxiv.org/abs/1606.01868)]
* Decoupled Neural Interfaces using Synthetic Gradients < > [[Paper](https://arxiv.org/abs/1608.05343)]

#### August-2016:

* Learning to learn by gradient descent by gradient descent < > [[Paper](https://arxiv.org/abs/1606.04474)]
* On Multiplicative Integration with Recurrent Neural Networks < > [[Paper](https://arxiv.org/abs/1606.06630)]
* Layer Normalization < > [[Paper](https://arxiv.org/abs/1607.06450)]
* Information-theoretical label embeddings for large-scale image classification < > [[Paper](https://arxiv.org/abs/1607.05691)]
* Structural-RNN: Deep Learning on Spatio-Temporal Graphs < > [[Paper](https://arxiv.org/abs/1511.05298)]

### June-July-2016:

* Sequence-to-Sequence Learning as Beam-Search Optimization < > [[Paper](https://arxiv.org/abs/1606.02960)]
* Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction < > [[Paper](https://arxiv.org/abs/1605.06353)]


#### till May-2016:

* Recurrent Batch Normalization < > [[Paper](https://arxiv.org/abs/1603.09025)]
* Gated Feedback Recurrent Neural Networks < > [[Paper](https://arxiv.org/abs/1502.02367)]
* Memory Networks < > [[Paper](https://arxiv.org/abs/1410.3916)]
* Neural Turing Machines < > [[Paper](https://arxiv.org/abs/1410.5401)]
* Neural GPUs Learn Algorithms < > [[Paper](https://arxiv.org/abs/1511.08228)]
* One-shot Learning with Memory-Augmented Neural Networks < > [[Paper](https://arxiv.org/abs/1605.06065)]
* Neural Programmer: Inducing Latent Programs with Gradient Descent < > [[Paper](https://arxiv.org/abs/1511.04834)]
* Neural Programmer-Interpreters < > [[Paper](https://arxiv.org/abs/1511.06279)]
* Reasoning about Entailment with Neural Attention < > [[Paper](https://arxiv.org/abs/1509.06664)]

#### March-2016:

* Recurrent Models of Visual Attention < > [[Paper](http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf)]
* Long Short-Term Memory-Networks for Machine Reading < > [[Paper](https://arxiv.org/abs/1601.06733)]
* Associative Long Short-Term Memory < > [[Paper](https://arxiv.org/abs/1602.03032)]
* Larger-Context Language Modelling < > [[Paper](https://arxiv.org/abs/1511.03729)]


-----

Prior to 2016:

#### [Deep Learning Summer 2015](https://github.com/0bserver07/Deep-Learning-Summer-2015#deep-learning-summer-2015)





----

Feel free to update the gradient of my notes, if it's not learning well ([Issues, comments, wrong format stuff](https://github.com/0bserver07/Mah-Paper-Notes/issues/new))
