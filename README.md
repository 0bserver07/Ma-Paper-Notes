**Brief Stacked 2 layered notes on papers around Machine Learning can be found below:**

----

* [Fully Character-Level Neural Machine Translation without Explicit Segmentation] (https://github.com/0bserver07/Mah-Paper-Notes/blob/master/notes/Fully%20Character-Level%20Neural%20Machine%20Translation%20without%20Explicit%20Segmentation.md) < > [[Paper](https://arxiv.org/abs/1610.03017v1)]



----

#### rest of Fall-2016

* Using Fast Weights to Attend to the Recent Past < > [[Paper](https://arxiv.org/abs/1610.06258)]
* Connecting Generative Adversarial Networks and Actor-Critic Methods < > [[Paper](https://arxiv.org/abs/1610.01945)]
* SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient < > [[Paper](https://arxiv.org/abs/1609.05473)]

#### Sep-2016

* Recurrent Highway Networks < > [[Paper](https://arxiv.org/abs/1607.03474)]
* Hierarchical Multiscale Recurrent Neural Networks < > [[Paper](https://arxiv.org/abs/1609.01704)]
* Unifying Count-Based Exploration and Intrinsic Motivation < > [[Paper](https://arxiv.org/abs/1606.01868)]
* Decoupled Neural Interfaces using Synthetic Gradients < > [[Paper](https://arxiv.org/abs/1608.05343)]

#### August-2016
* Learning to learn by gradient descent by gradient descent < > [[Paper](https://arxiv.org/abs/1606.04474)]
* On Multiplicative Integration with Recurrent Neural Networks < > [[Paper](https://arxiv.org/abs/1606.06630)]
* Layer Normalization < > [[Paper](https://arxiv.org/abs/1607.06450)]
* Information-theoretical label embeddings for large-scale image classification < > [[Paper](https://arxiv.org/abs/1607.05691)]
* Structural-RNN: Deep Learning on Spatio-Temporal Graphs < > [[Paper](https://arxiv.org/abs/1511.05298)]


#### till May-2016

* Recurrent Batch Normalization < > [[Paper](https://arxiv.org/abs/1603.09025)]
* Gated Feedback Recurrent Neural Networks < > [[Paper](https://arxiv.org/abs/1502.02367)]
* Memory Networks < > [[Paper](https://arxiv.org/abs/1410.3916)]
* Neural Turing Machines < > [[Paper](https://arxiv.org/abs/1410.5401)]
* Neural GPUs Learn Algorithms < > [[Paper](https://arxiv.org/abs/1511.08228)]
* One-shot Learning with Memory-Augmented Neural Networks < > [[Paper](https://arxiv.org/abs/1605.06065)]
* Neural Programmer: Inducing Latent Programs with Gradient Descent < > [[Paper](https://arxiv.org/abs/1511.04834)]
* Neural Programmer-Interpreters < > [[Paper](https://arxiv.org/abs/1511.06279)]

#### March-2016

* Recurrent Models of Visual Attention < > [[Paper](http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf)]
* Long Short-Term Memory-Networks for Machine Reading < > [[Paper](https://arxiv.org/abs/1601.06733)]
* Associative Long Short-Term Memory < > [[Paper](https://arxiv.org/abs/1602.03032)]






----

Feel free to update the gradient of my notes, if it's not learning well ([Issues, comments, wrong format stuff](https://github.com/0bserver07/Mah-Paper-Notes/issues/new))
