{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ***\"CAUSALITY FOR MACHINE LEARNING\"*** by Bernhard Schölkopf\n",
    "\n",
    "> \"It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Problem: \n",
    "> \"if we compare what machine learning can do to what animals accomplish, we observe that the former is rather bad at some crucial feats where animals excel.\"\n",
    "\n",
    "- generalization\n",
    "    - *\"interventions in the world, domain shifts, temporal structure —- by and large, we consider these factors a nuisance and try to engineer them away.\"*\n",
    "- thinking\n",
    "    -  *\"thinking in the sense of Konrad Lorenz, i.e., acting in an imagined space.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The Mechanization of Information Processing\n",
    "- From Statistical to Causal Models\n",
    "    - Structural causal models (SCMs)\n",
    "- Levels of Causal Modelling\n",
    "- Independent Causal Mechanisms\n",
    "    - Independent Causal Mechanisms (ICM) Principle.\n",
    "    - Measures of dependence of mechanisms\n",
    "    - Algorithmic independence\n",
    "- Cause-Effect Discovery\n",
    "- Half-Sibling Regression and Exoplanet Detection\n",
    "- Invariance, Robustness, and Semi-Supervised Learning\n",
    "    - Semi-supervised learning (SSL)\n",
    "    - Adversarial vulnerability\n",
    "    - Multi-task learning\n",
    "    - Reinforcement Learning\n",
    "- Causal Representation Learning\n",
    "    - Learning transferable mechanisms\n",
    "- Learning disentangled representations\n",
    "- Bernhard's Personal Notes and Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. The Mechanization of Information Processing\n",
    "\n",
    "- The first industrial revolution:\n",
    "    - Generate and convert forms of **\"energy\"**\n",
    "- The Digital to AI revolution:\n",
    "    - ***\"Cybernetics\"***. It replaced energy by information.\n",
    "\n",
    "> When machine learning is applied in industry, we often convert user data into predictions about future user behavior and thus money. Money may ultimately be a form of information —- a view not inconsistent with the idea of bitcoins generated by solving cryptographic problems. The first industrial revolutions rendered energy a universal currency (Smil, 2017); the same may be happening to information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar to the energy reveolution:\n",
    "\n",
    "- Industry, (Hardware and Software):\n",
    "    > the first one built on the advent of electronic computers, the development of high level programming languages, and the birth of the field of computer science, engendered by the vision to create AI by manipulation of symbols.\n",
    "\n",
    "- Learning, (information extraction):\n",
    "    > The second one, which we are currently experiencing, relies upon learning. It allows to extract information also from unstructured data, and it automatically infers rules from data rather than relying on humans to conceive of and program these rules.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Judea Pearl: classic AI with probability theory (Pearl, 1988)\n",
    "    > graphical models\n",
    "\n",
    "- Vladmir Vapnik,supervised machine learning (Vapnik, 1998)\n",
    "    > IT industry, and the second part is transforming IT companies into “AI first” as well as creating an industry around data collection and “clickwork.” While the latter provides labelled data for the current workhorse of AI, supervised machine learning (Vapnik, 1998)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- (Klein, 1872; MacLane, 1971)\n",
    "    > Symmetry transformations and defining objects by their behavior:\n",
    "    \n",
    "- (Chen and Cheung, 2018; Dai, 2018)\n",
    "    > In the current data-driven phase of this revolution, China is beginning to use machine learning to observe and incentivize citizens to behave in ways deemed beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. From Statistical to Causal Models\n",
    "\n",
    "- Methods driven by independent and identically distributed (IID) data, eg (LeCun et al., 2015)\n",
    "    > ***\"a collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent.\"***\n",
    "\n",
    "- trends:\n",
    "    1. we have massive amounts of data, often from simulations or large scale human labeling\n",
    "    2. we use high capacity machine learning systems (i.e., complex function classes with many adjustable parameters) \n",
    "    3. we employ high performance computing systems (often ignored, but crucial when it comes to causality) \n",
    "    4. the problems are IID (independent and identically distributed).\n",
    "        - (e.g., image recognition using benchmark datasets)\n",
    "        - artificially made IID, e.g.. such as DeepMind’s “experience replay” (Mnih et al., 2015)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Why IID?\n",
    "\n",
    "-  IID data + statistical learning theory = strong universal consistency results\n",
    "    - Nearest Neighbor Classifiers \n",
    "    - Support Vector Machines \n",
    "\n",
    "(Vapnik, 1998; Schölkopf and Smola, 2002; Steinwart and Christmann, 2008)\n",
    "\n",
    "Otherwise:\n",
    "- “adversarial vulnerability”\n",
    "- “defense mechanisms”\n",
    "\n",
    "Problem in recommender systems:\n",
    "- Suppose Alice is looking for a laptop keyboard on the internet (i.e., any keyboard), and the web shop’s recommendation system suggests that she should buy a laptop to go along with the keyboard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- (the mutual information), so the directionality of cause and effect is lost.\n",
    "> Recommending an item to buy constitutes an intervention in a system, taking us outside the IID setting. We no longer work with the observational distribution, but a distribution where certain variables or mechanisms have changed. This is the realm of causality.\n",
    "\n",
    "- Reichenbach (1956) connection between causality and statistical dependence. \n",
    "\n",
    "***\"Common Cause Principle\"***: if two observables X and Y are statistically dependent, then there exists a variable Z that causally influences both and explains all the dependence in the sense of making them independent when conditioned on Z."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\"Structural causal models (SCMs)\"***: in case of estimating functions rather than probability distributions. We are given a set of ***observables*** X1 , . . . , Xn (modelled as random variables) associated with the vertices of a directed acyclic graph (DAG) G. We assume that each observable is the result of an assignment\n",
    "\n",
    "```latex\n",
    "Xi := fi(PAi,Ui), (i = 1,...,n),\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> 1. SCM language makes it straightforward to formalize interventions as operations that modify a subset of assignments (1), e.g., changing Ui, or setting fi (and thus Xi) to a constant (Pearl, 2009a; Spirtes et al., 2000). \n",
    "\n",
    "> 2. Second, the graph structure along with the joint independence of the noises implies a canonical factorization of the joint distribution entailed by (1) into causal conditionals that we will refer to as the causal (or disentangled) factorization,\n",
    "\n",
    "![image.png](./eq2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4. Levels of Causal Modelling\n",
    "\n",
    "![image.png](./tbl1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5 Independent Causal Mechanisms\n",
    "\n",
    "> disentangled factorization (2) of the joint distribution p(X1, . . . , Xn). This factorization according to the causal graph is always possible when the Ui are independent\n",
    "\n",
    "![image.png](./chair.png)\n",
    "\n",
    "- One can express the above insights as follows (Schölkopf et al., 2012; Peters et al., 2017):\n",
    "\n",
    "> ***\"Independent Causal Mechanisms (ICM) Principle. The causal generative process of a system’s variables is composed of autonomous modules that do not inform or influence each other.\"***\n",
    "\n",
    "> \"In the probabilistic case, this means that the conditional distribution of each variable given its causes (i.e., its mechanism) does not inform or influence the other mechanisms.\n",
    "\n",
    "- ***\"Measures of dependence of mechanisms\"***\n",
    " > Algorithmic independence: links between causal and statistical structures.\n",
    "     - ***causal structure***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
